# üöÄ Working with Large Datasets (>1,000 Rows) - FREE Solution

## Your Question: "What if I want to work on data with > 1,000 rows / large datasets?"

Great question! Here's a comprehensive **FREE strategy** for handling datasets with 10k, 100k, or even 1M+ rows.

---

## Current Limitations & Why They Exist

### What Works Now:
- ‚úÖ **AI Analysis**: ALL rows (no limit) - uses database
- ‚úÖ **Data Quality**: ALL rows (no limit) - server-side
- ‚ùå **Visualizations**: Only 1,000 rows - localStorage limit

### Why 1,000 Row Limit?
1. **localStorage**: ~10MB limit per domain
2. **Browser Performance**: Rendering 100k points = browser freeze
3. **Memory**: Large datasets crash browser tabs

---

## Solution: Smart Strategies (Still 100% FREE!)

### Strategy 1: **Intelligent Downsampling** ‚≠ê RECOMMENDED

Instead of showing first 1,000 rows, show **representative samples** that give accurate visualization.

#### Implementation (FREE):

```typescript
// File: utils/dataSampling.ts

/**
 * Largest Triangle Three Buckets (LTTB) Algorithm
 * Industry standard for time-series downsampling
 * Used by Grafana, Plotly, and other viz tools
 * 100% FREE - no API calls
 */

export function downsampleLTTB(data: any[], targetPoints: number, xKey: string, yKey: string): any[] {
  if (data.length <= targetPoints) return data;

  const sampled: any[] = [];
  const bucketSize = (data.length - 2) / (targetPoints - 2);

  // Always include first point
  sampled.push(data[0]);

  for (let i = 0; i < targetPoints - 2; i++) {
    const avgRangeStart = Math.floor((i + 1) * bucketSize) + 1;
    const avgRangeEnd = Math.floor((i + 2) * bucketSize) + 1;
    const avgRangeLength = avgRangeEnd - avgRangeStart;

    // Calculate average point in next bucket
    let avgX = 0;
    let avgY = 0;
    for (let j = avgRangeStart; j < avgRangeEnd; j++) {
      avgX += j;
      avgY += Number(data[j][yKey] || 0);
    }
    avgX /= avgRangeLength;
    avgY /= avgRangeLength;

    // Find point in current bucket with largest triangle area
    const rangeStart = Math.floor(i * bucketSize) + 1;
    const rangeEnd = Math.floor((i + 1) * bucketSize) + 1;

    let maxArea = -1;
    let maxAreaPoint = data[rangeStart];

    const pointAX = sampled[sampled.length - 1][xKey] || sampled.length - 1;
    const pointAY = Number(sampled[sampled.length - 1][yKey] || 0);

    for (let j = rangeStart; j < rangeEnd; j++) {
      const pointX = data[j][xKey] || j;
      const pointY = Number(data[j][yKey] || 0);

      // Calculate triangle area
      const area = Math.abs(
        (pointAX - avgX) * (pointY - pointAY) -
        (pointAX - pointX) * (avgY - pointAY)
      ) * 0.5;

      if (area > maxArea) {
        maxArea = area;
        maxAreaPoint = data[j];
      }
    }

    sampled.push(maxAreaPoint);
  }

  // Always include last point
  sampled.push(data[data.length - 1]);

  return sampled;
}

/**
 * Smart sampling for categorical data
 */
export function smartCategoricalSample(data: any[], maxPoints: number = 1000): any[] {
  if (data.length <= maxPoints) return data;

  // Strategy: Include all unique categories, sample within each
  const samples: any[] = [];
  const interval = Math.ceil(data.length / maxPoints);

  for (let i = 0; i < data.length; i += interval) {
    samples.push(data[i]);
  }

  // Always include last row
  if (samples[samples.length - 1] !== data[data.length - 1]) {
    samples.push(data[data.length - 1]);
  }

  return samples;
}
```

#### Update Upload API:

```typescript
// In app/api/upload/route.ts

import { downsampleLTTB, smartCategoricalSample } from '@/utils/dataSampling';

// For datasets > 1,000 rows, apply smart sampling
if (parsedData.rows.length > 1000) {
  // Detect if time-series or categorical
  const hasNumericColumn = parsedData.columns.some(col =>
    parsedData.types[parsedData.columns.indexOf(col)] === 'number'
  );

  const sampledData = hasNumericColumn
    ? downsampleLTTB(parsedData.rows, 1000, parsedData.columns[0], parsedData.columns[1])
    : smartCategoricalSample(parsedData.rows, 1000);

  // Send sampled data instead of first 1,000
  fullDataForClient = sampledData;
}
```

**Benefits:**
- ‚úÖ 10,000 rows ‚Üí 1,000 points that look like 10,000!
- ‚úÖ Preserves trends, peaks, and patterns
- ‚úÖ Better than "first 1,000 rows"
- ‚úÖ Still FREE - no API costs

---

### Strategy 2: **Dynamic Data Loading from Database** ‚≠ê BEST FOR HUGE DATASETS

For datasets > 10,000 rows, don't use localStorage at all. Fetch aggregated data on-demand.

#### Implementation:

```typescript
// File: app/api/chart-data/route.ts

import { NextRequest, NextResponse } from 'next/server';
import { getServiceSupabase } from '@/lib/supabase';

export async function POST(request: NextRequest) {
  const { datasetId, userId, chartType, xColumn, yColumn, aggregation } = await request.json();

  const supabase = getServiceSupabase();

  // Fetch dataset metadata
  const { data: dataset } = await supabase
    .from('datasets')
    .select('*')
    .eq('id', datasetId)
    .eq('user_id', userId)
    .single();

  if (!dataset) {
    return NextResponse.json({ error: 'Dataset not found' }, { status: 404 });
  }

  // Parse ALL rows from database
  const allRows = dataset.data_rows.map((row: string) =>
    typeof row === 'string' ? JSON.parse(row) : row
  );

  // Apply server-side aggregation/sampling
  let chartData;

  if (chartType === 'line' || chartType === 'area') {
    // For time-series: Use LTTB downsampling
    chartData = downsampleLTTB(allRows, 1000, xColumn, yColumn);
  } else if (chartType === 'bar') {
    // For bar charts: Group by category and aggregate
    chartData = aggregateByCategory(allRows, xColumn, yColumn, aggregation || 'sum');
  } else if (chartType === 'pie') {
    // For pie: Top N categories
    chartData = getTopCategories(allRows, xColumn, yColumn, 10);
  }

  return NextResponse.json({
    data: chartData,
    originalRowCount: allRows.length,
    displayedPoints: chartData.length,
  });
}

// Helper: Aggregate by category
function aggregateByCategory(data: any[], groupBy: string, valueColumn: string, method: 'sum' | 'avg' | 'count'): any[] {
  const groups = new Map<string, number[]>();

  data.forEach(row => {
    const key = String(row[groupBy] || 'Unknown');
    const value = Number(row[valueColumn] || 0);

    if (!groups.has(key)) {
      groups.set(key, []);
    }
    groups.get(key)!.push(value);
  });

  const result: any[] = [];
  groups.forEach((values, key) => {
    let aggregatedValue;
    if (method === 'sum') {
      aggregatedValue = values.reduce((a, b) => a + b, 0);
    } else if (method === 'avg') {
      aggregatedValue = values.reduce((a, b) => a + b, 0) / values.length;
    } else {
      aggregatedValue = values.length;
    }

    result.push({
      [groupBy]: key,
      [valueColumn]: aggregatedValue,
    });
  });

  return result.sort((a, b) => b[valueColumn] - a[valueColumn]);
}

// Helper: Get top N categories
function getTopCategories(data: any[], categoryColumn: string, valueColumn: string, topN: number): any[] {
  const aggregated = aggregateByCategory(data, categoryColumn, valueColumn, 'sum');
  return aggregated.slice(0, topN);
}
```

**Update Analytics Page:**

```typescript
// In app/analytics/page.tsx

const [chartData, setChartData] = useState<any[]>([]);
const [loading, setLoading] = useState(false);

const fetchChartData = async (chartType: string, xCol: string, yCol: string) => {
  setLoading(true);

  const response = await fetch('/api/chart-data', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      datasetId: uploadedData.datasetId,
      userId: user.id,
      chartType,
      xColumn: xCol,
      yColumn: yCol,
      aggregation: 'sum',
    }),
  });

  const data = await response.json();
  setChartData(data.data);
  setLoading(false);
};

// Use chartData instead of uploadedData.preview.fullData
<ChartDisplay
  type="bar"
  data={chartData.length > 0 ? chartData : uploadedData?.preview?.fullData || []}
  title="Bar Chart"
/>
```

**Benefits:**
- ‚úÖ Works with 1M+ rows
- ‚úÖ No localStorage limits
- ‚úÖ Server-side aggregation = fast
- ‚úÖ Still FREE (Supabase free tier)

---

### Strategy 3: **Column Selector** (User Chooses What to Visualize)

Let users select specific columns to visualize instead of loading everything.

```typescript
// Add to Analytics/Visualizations page

const [selectedColumns, setSelectedColumns] = useState<string[]>([]);

<select
  multiple
  onChange={(e) => setSelectedColumns(Array.from(e.target.selectedOptions, opt => opt.value))}
>
  {uploadedData?.preview?.columns.map(col => (
    <option key={col} value={col}>{col}</option>
  ))}
</select>

// Only fetch/display selected columns
const filteredData = allData.map(row => {
  const filtered: any = {};
  selectedColumns.forEach(col => {
    filtered[col] = row[col];
  });
  return filtered;
});
```

---

### Strategy 4: **Pagination for Data Tables**

For viewing raw data, use pagination instead of loading all rows.

```typescript
const ROWS_PER_PAGE = 100;
const [currentPage, setCurrentPage] = useState(1);

const displayedRows = allRows.slice(
  (currentPage - 1) * ROWS_PER_PAGE,
  currentPage * ROWS_PER_PAGE
);

<table>
  {displayedRows.map(row => <tr>...</tr>)}
</table>

<div>
  Page {currentPage} of {Math.ceil(allRows.length / ROWS_PER_PAGE)}
  <button onClick={() => setCurrentPage(p => p - 1)}>Previous</button>
  <button onClick={() => setCurrentPage(p => p + 1)}>Next</button>
</div>
```

---

## Recommended Approach by Dataset Size

| Dataset Size | Strategy | Why |
|--------------|----------|-----|
| **< 1,000 rows** | Current (all in localStorage) | Fast, simple |
| **1k - 10k rows** | LTTB Downsampling | Preserves visual accuracy |
| **10k - 100k rows** | Database + Aggregation API | Too large for browser |
| **100k - 1M rows** | DB Aggregation + Column Selection | Need server-side processing |
| **> 1M rows** | DB Aggregation + Chunked Loading | Enterprise-level approach |

---

## Quick Implementation Plan

### Phase 1: Add LTTB Downsampling (2 hours)
1. Create `utils/dataSampling.ts`
2. Update upload API to use LTTB for >1k rows
3. Test with 10k row dataset

### Phase 2: Add Chart Data API (4 hours)
1. Create `/api/chart-data/route.ts`
2. Update Analytics page to fetch on-demand
3. Add loading states

### Phase 3: Add Column Selector (2 hours)
1. Add dropdown for column selection
2. Filter data based on selection
3. Update charts dynamically

---

## Example: 100,000 Row Dataset

### Current Approach (Limited):
```
Upload 100,000 rows
‚Üì
Send first 1,000 to localStorage
‚Üì
Charts show first 1,000 (may miss important patterns!)
```

### With LTTB (Better):
```
Upload 100,000 rows
‚Üì
LTTB downsamples to 1,000 points (representative!)
‚Üì
Charts show patterns from ALL 100k rows
```

### With DB Aggregation (Best):
```
Upload 100,000 rows ‚Üí Store in database
‚Üì
User selects chart type
‚Üì
Fetch aggregated data (e.g., daily averages)
‚Üì
Charts show 365 points representing 100k rows
```

---

## Performance Comparison

### First 1,000 Rows (Current):
- ‚ùå Misses patterns in rows 1,001 - 100,000
- ‚ùå Biased toward beginning of dataset
- ‚ùå Inaccurate for sorted data

### LTTB Downsampling:
- ‚úÖ Visually identical to full dataset
- ‚úÖ Preserves peaks, valleys, trends
- ‚úÖ Works with any data distribution

### DB Aggregation:
- ‚úÖ Works with unlimited data size
- ‚úÖ Real-time aggregation
- ‚úÖ User controls what to see

---

## Cost Analysis (All FREE!)

| Approach | Storage | Compute | API Calls | Total Cost |
|----------|---------|---------|-----------|------------|
| **Current** | localStorage (FREE) | Client (FREE) | 0 | $0 |
| **LTTB** | localStorage (FREE) | Server (FREE) | 0 | $0 |
| **DB Aggregation** | Supabase (FREE tier) | Server (FREE) | 0 | $0 |

**All strategies are 100% FREE!**

---

## What I Recommend

### For Your App (Best Balance):

1. **Keep current approach for ‚â§ 1,000 rows** (fast, simple)

2. **Add LTTB for 1k - 10k rows** (easy to implement, big improvement)

3. **Add DB aggregation for > 10k rows** (enterprise-level feature)

4. **Add column selector for all sizes** (better UX)

---

## Implementation Priority

### Quick Win (1-2 hours):
```typescript
// Just add LTTB to upload API
if (parsedData.rows.length > 1000) {
  fullDataForClient = downsampleLTTB(parsedData.rows, 1000, cols[0], cols[1]);
}
```

### Medium Term (1 day):
- Create chart-data API endpoint
- Add loading states
- Test with 50k row dataset

### Long Term (2-3 days):
- Add column selection UI
- Add aggregation options (sum, avg, count)
- Add data pagination for tables

---

## Want Me to Implement This?

I can implement:

1. ‚úÖ LTTB downsampling algorithm (30 min)
2. ‚úÖ Chart data API endpoint (1 hour)
3. ‚úÖ Column selector UI (30 min)
4. ‚úÖ Update pages to use new APIs (1 hour)

All **100% FREE** - no additional costs!

Just say the word and I'll start! üöÄ

---

## Summary

**Your Question:** "What if I want to work on data with > 1,000 rows?"

**Answer:**
- ‚úÖ AI already works with unlimited rows (uses database)
- ‚úÖ For visualizations, use LTTB downsampling or DB aggregation
- ‚úÖ All solutions are FREE
- ‚úÖ Can handle 1M+ rows with right strategy
- ‚úÖ I can implement this for you now!

Last updated: 2025-11-29
